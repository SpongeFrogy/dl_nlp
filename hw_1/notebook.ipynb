{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import corus\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load `lenta-ru-news`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already loaded.\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 100_000 # cut off size for faster performance\n",
    "\n",
    "path = 'lenta-ru-news.csv.gz'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    !wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
    "else:\n",
    "    print(\"Already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load lenta: 739351it [00:20, 36232.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153198</th>\n",
       "      <td>EgyptAir объявила о подорожании билетов</td>\n",
       "      <td>Египетский перевозчик EgyptAir сообщил о возмо...</td>\n",
       "      <td>Путешествия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169154</th>\n",
       "      <td>Глава Красногорского района Подмосковья ушел в...</td>\n",
       "      <td>Глава Красногорского района Московской области...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83745</th>\n",
       "      <td>Милонов предложил запретить россиянам сидеть в...</td>\n",
       "      <td>Депутат Виталий Милонов внес в Госдуму законоп...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10029</th>\n",
       "      <td>Женщинам в детородном возрасте разрешили посещ...</td>\n",
       "      <td>Верховный суд Индии разрешил женщинам в фертил...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>Россиянам пообещали дешевый хлеб</td>\n",
       "      <td>Россиянам не стоит бояться роста цен на хлеб —...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45187</th>\n",
       "      <td>МОК наказал Мутко</td>\n",
       "      <td>Российский вице-премьер Виталий Мутко пожизнен...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399176</th>\n",
       "      <td>Жара резко увеличила энергопотребление в Москве</td>\n",
       "      <td>Потребление электроэнергии в Москве в последни...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66430</th>\n",
       "      <td>Федор Емельяненко и Милонов снимутся во «Лжи М...</td>\n",
       "      <td>На Урале снимут картину «Ложь Матильды», посвя...</td>\n",
       "      <td>Культура</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78735</th>\n",
       "      <td>Трамп высмеял противившихся увольнению директо...</td>\n",
       "      <td>Президент США Дональд Трамп опубликовал подбор...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102670</th>\n",
       "      <td>В Москве подтвердили передачу Сербии шести ист...</td>\n",
       "      <td>Весной 2017 года Сербия получит шесть истребит...</td>\n",
       "      <td>Силовые структуры</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "153198           EgyptAir объявила о подорожании билетов    \n",
       "169154  Глава Красногорского района Подмосковья ушел в...   \n",
       "83745   Милонов предложил запретить россиянам сидеть в...   \n",
       "10029   Женщинам в детородном возрасте разрешили посещ...   \n",
       "6445                     Россиянам пообещали дешевый хлеб   \n",
       "45187                                   МОК наказал Мутко   \n",
       "399176    Жара резко увеличила энергопотребление в Москве   \n",
       "66430   Федор Емельяненко и Милонов снимутся во «Лжи М...   \n",
       "78735   Трамп высмеял противившихся увольнению директо...   \n",
       "102670  В Москве подтвердили передачу Сербии шести ист...   \n",
       "\n",
       "                                                     text              topic  \n",
       "153198  Египетский перевозчик EgyptAir сообщил о возмо...        Путешествия  \n",
       "169154  Глава Красногорского района Московской области...             Россия  \n",
       "83745   Депутат Виталий Милонов внес в Госдуму законоп...             Россия  \n",
       "10029   Верховный суд Индии разрешил женщинам в фертил...                Мир  \n",
       "6445    Россиянам не стоит бояться роста цен на хлеб —...          Экономика  \n",
       "45187   Российский вице-премьер Виталий Мутко пожизнен...              Спорт  \n",
       "399176  Потребление электроэнергии в Москве в последни...          Экономика  \n",
       "66430   На Урале снимут картину «Ложь Матильды», посвя...           Культура  \n",
       "78735   Президент США Дональд Трамп опубликовал подбор...                Мир  \n",
       "102670  Весной 2017 года Сербия получит шесть истребит...  Силовые структуры  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = corus.load_lenta(path)\n",
    "def parse(record) -> dict[str, str]:\n",
    "    return dict(title=record.title, text=record.text, topic=record.topic)\n",
    "df = pd.DataFrame(list(tqdm(map(lambda r: parse(r), records), desc='load lenta')))\n",
    "df = df.sample(DATA_SIZE, random_state=42)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clear up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание\n",
    "1. Normalization `text = re.sub(r'[^а-яё\\s]', '', text.lower())`\n",
    "2. Lemmatization `lemma = morph.parse(word)[0].normal_form` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Initial__________\n",
      "Tokens unique count: 1227\n",
      "Египетский перевозчик EgyptAir сообщил о возможном повышении стоимости билетов на свои международные...\n",
      "_______Normalization_______\n",
      "Tokens unique count: 1074, skipped 12.5%\n",
      "египетский перевозчик  сообщил о возможном повышении стоимости билетов на свои международные рейсы и...\n",
      "___________Lemma___________\n",
      "Tokens unique count: 851, reduced 20.8%\n",
      "египетский перевозчик сообщить о возможный повышение стоимость билет на свой международный рейс изз ...\n",
      "______Skip Stop Words______\n",
      "Tokens unique count: 792, skipped 6.9%\n",
      "египетский перевозчик сообщить возможный повышение стоимость билет свой международный рейс изз девал...\n"
     ]
    }
   ],
   "source": [
    "init = ' '.join(df['text'].iloc[0:10])\n",
    "example = init\n",
    "\n",
    "print(f'__________Initial__________\\nTokens unique count: {len(set(example.split()))}\\n{example[:100]}...')\n",
    "__old_tokens_cnt = len(set(example.split()))\n",
    "example = re.sub(r'[^а-яё\\s]', '', example.lower())\n",
    "print(f'_______Normalization_______\\nTokens unique count: {len(set(example.split()))}, skipped {(1 - len(set(example.split()))/__old_tokens_cnt) * 100:.1f}%\\n{example[:100]}...')\n",
    "__old_tokens_cnt = len(set(example.split()))\n",
    "example = ' '.join(map(lambda w: morph.parse(w)[0].normal_form, example.split()))\n",
    "print(f'___________Lemma___________\\nTokens unique count: {len(set(example.split()))}, reduced {(1 - len(set(example.split()))/__old_tokens_cnt) * 100:.1f}%\\n{example[:100]}...')\n",
    "__old_tokens_cnt = len(set(example.split()))\n",
    "example = ' '.join(filter(lambda w: w not in russian_stopwords, example.split()))\n",
    "print(f'______Skip Stop Words______\\nTokens unique count: {len(set(example.split()))}, skipped {(1 - len(set(example.split()))/__old_tokens_cnt) * 100:.1f}%\\n{example[:100]}...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как для задачи нам необходим смысл текста, то нам важно убрать все ненужное и сохранить смысл текста\n",
    " - Нормализация  `re.sub(r'[^а-яё\\s]', '', example.lower())` позволяет привести слова к единому виду и убрать не смысловые токены\n",
    " - Лемматизация позволяет избавиться от проблемы разных форм слов и тем самым я увеличиваю количество вхождения каждого токена, увеличиваю количество связей тежду текстами через слова\n",
    " - убираю не смысловые стоп слова для ускорения обучения и повышения качества н-грамм "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "# кэшируем ответы, чтобы препроцессить не бесконечность времени\n",
    "@functools.lru_cache(maxsize=10_000)\n",
    "def normalize(token):\n",
    "    return morph.parse(token)[0].normal_form\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text.lower())\n",
    "    return ' '.join(\n",
    "        filter(\n",
    "            lambda token: token not in russian_stopwords,\n",
    "            map(\n",
    "                normalize,\n",
    "                text.split()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "assert preprocess_text(init) == example, 'processing error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess text: 100%|██████████| 100000/100000 [05:57<00:00, 279.45it/s]\n"
     ]
    }
   ],
   "source": [
    "df['processed_text'] = list(tqdm(map(\n",
    "    preprocess_text, df['text'].to_list()), desc='preprocess text', total=len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Drop not representative classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете есть нерепрезентативные классы, количество вхождений которых не позволяет качественно обучить классификатор (на этих классах), скнитем их в кучу `other`. Таким образом мы сохраняем количество данных и сохраняем изначальное распределение, что очень полезно при раскатывании модели в реальной жизни.\n",
    "\n",
    "Из эмпирических соображений считаем, что ***минимальное число наблюдений на класс, необходимое для обучения, будет 10% от размера датасета***   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal topic frequency: 1000\n"
     ]
    }
   ],
   "source": [
    "MIN_TOPIC_FREQ = int(.01*len(df))\n",
    "assert MIN_TOPIC_FREQ > 3, 'Too small MIN_TOPIC_FREQ for split train/val/test'\n",
    "print(f'Minimal topic frequency: {MIN_TOPIC_FREQ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics with counts before preprocessing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               21871\n",
       "Мир                  18494\n",
       "Экономика            10737\n",
       "Спорт                 8632\n",
       "Культура              7337\n",
       "Наука и техника       7129\n",
       "Бывший СССР           7100\n",
       "Интернет и СМИ        6181\n",
       "Из жизни              3718\n",
       "Дом                   2891\n",
       "Силовые структуры     2661\n",
       "Ценности              1079\n",
       "Бизнес                 967\n",
       "Путешествия            855\n",
       "69-я параллель         178\n",
       "Крым                    82\n",
       "Культпросвет            45\n",
       "                        23\n",
       "Легпром                 10\n",
       "Библиотека               8\n",
       "Сочи                     1\n",
       "Оружие                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Topics with counts before preprocessing')\n",
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Strategy(Enum):\n",
    "    \"\"\"\n",
    "    Тут отразил разные стратегии для решения проблемы нерепрезентативности классов\n",
    "    \"\"\"\n",
    "    NONE = 0  # ничего не делаем\n",
    "    DROP = 1  # опускаем нерепрезентативные классы\n",
    "    REPLACE_WITH_OTHER = 2  # скидываем из в кучу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY = Strategy.REPLACE_WITH_OTHER\n",
    "\n",
    "match STRATEGY:\n",
    "    case Strategy.NONE:\n",
    "        pass\n",
    "    case Strategy.DROP:\n",
    "        _counts = df['topic'].value_counts()\n",
    "        _keep = (_counts[_counts > MIN_TOPIC_FREQ]).index.to_list()\n",
    "        df = df[df['topic'].apply(lambda x: x in _keep)]\n",
    "    case Strategy.REPLACE_WITH_OTHER:\n",
    "        _counts = df['topic'].value_counts()\n",
    "        _keep = (_counts[_counts > MIN_TOPIC_FREQ]).index.to_list()\n",
    "        df['topic'] = df['topic'].apply(\n",
    "            lambda x: 'other' if x not in _keep else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics with counts after preprocessing with strategy `REPLACE_WITH_OTHER`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               21871\n",
       "Мир                  18494\n",
       "Экономика            10737\n",
       "Спорт                 8632\n",
       "Культура              7337\n",
       "Наука и техника       7129\n",
       "Бывший СССР           7100\n",
       "Интернет и СМИ        6181\n",
       "Из жизни              3718\n",
       "Дом                   2891\n",
       "Силовые структуры     2661\n",
       "other                 2170\n",
       "Ценности              1079\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Topics with counts after preprocessing with strategy `{STRATEGY.name}`')\n",
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Train/Test/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000, 0.60%\n",
      "Val size: 20000, 0.20%\n",
      "Test size: 20000, 0.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['processed_text']\n",
    "y = df['topic']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "assert len(X_train) + len(X_val) + len(X_test) == len(X)\n",
    "assert len(y_train) + len(y_val) + len(y_test) == len(y)\n",
    "assert np.all(sorted(y.unique()) == sorted(y_train.unique()))\n",
    "assert np.all(sorted(y.unique()) == sorted(y_test.unique()))\n",
    "assert np.all(sorted(y.unique()) == sorted(y_val.unique()))\n",
    "\n",
    "print(f'Train size: {len(X_train)}, {len(X_train) / len(X):.2f}%')\n",
    "print(f'Val size: {len(X_val)}, {len(X_val) / len(X):.2f}%')\n",
    "print(f'Test size: {len(X_test)}, {len(X_test) / len(X):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score (accuracy): 0.2187\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            other       0.00      0.00      0.00       434\n",
      "      Бывший СССР       0.00      0.00      0.00      1420\n",
      "              Дом       0.00      0.00      0.00       578\n",
      "         Из жизни       0.00      0.00      0.00       743\n",
      "   Интернет и СМИ       0.00      0.00      0.00      1236\n",
      "         Культура       0.00      0.00      0.00      1468\n",
      "              Мир       0.00      0.00      0.00      3699\n",
      "  Наука и техника       0.00      0.00      0.00      1426\n",
      "           Россия       0.22      1.00      0.36      4374\n",
      "Силовые структуры       0.00      0.00      0.00       532\n",
      "            Спорт       0.00      0.00      0.00      1726\n",
      "         Ценности       0.00      0.00      0.00       216\n",
      "        Экономика       0.00      0.00      0.00      2148\n",
      "\n",
      "         accuracy                           0.22     20000\n",
      "        macro avg       0.02      0.08      0.03     20000\n",
      "     weighted avg       0.05      0.22      0.08     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "y_val_pred = dummy.predict(X_val)\n",
    "\n",
    "print(f\"Test score (accuracy): {dummy.score(X_val, y_val)}\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут надо обговорить, что параметр `max_features` для векторизаторов отвечает за количество токенов с максимально частотой в обучающем датасете. Ясно, что если токен встречается единицы раз, то он нам бесполезен, поэтому надо этот параметр брать не из головы, а ходьбы посмотреть, что в модель не попадают очень редкие токены. В коде ниже хочу отразить, что этот параметр отвечает здравому смыслу (тут я не говорю, что 18 - самая лучшая оценка минимального числа вхождения токена, это число выглядит неплохой начальной оценкой) \n",
    "\n",
    "\n",
    "p. s. Этот параметр  также можно подбирать как перцентиль то количеств вхождений каждого токена (например $\n",
    "P_{90}$)\n",
    "\n",
    "p. p. s. Явное ограничение количество токенов дает возможность явно ограничить вес модели, что супер полезно в реальных задачах с большими данными.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum common token count: 18\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_FEATURES = 20_000\n",
    "\n",
    "token_counter = Counter()\n",
    "for text in X_train:\n",
    "    token_counter.update(text.split())\n",
    "\n",
    "_, min_common_token_count = token_counter.most_common(MAX_FEATURES)[-1]\n",
    "assert min_common_token_count > 1, 'Too small min_common_token_count'\n",
    "print(f'Minimum common token count: {min_common_token_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            other       0.54      0.50      0.52       434\n",
      "      Бывший СССР       0.79      0.80      0.79      1420\n",
      "              Дом       0.82      0.85      0.83       578\n",
      "         Из жизни       0.55      0.69      0.61       743\n",
      "   Интернет и СМИ       0.73      0.70      0.72      1236\n",
      "         Культура       0.86      0.88      0.87      1468\n",
      "              Мир       0.79      0.78      0.79      3699\n",
      "  Наука и техника       0.81      0.83      0.82      1426\n",
      "           Россия       0.81      0.75      0.78      4374\n",
      "Силовые структуры       0.55      0.63      0.59       532\n",
      "            Спорт       0.96      0.96      0.96      1726\n",
      "         Ценности       0.85      0.83      0.84       216\n",
      "        Экономика       0.81      0.84      0.83      2148\n",
      "\n",
      "         accuracy                           0.79     20000\n",
      "        macro avg       0.76      0.77      0.76     20000\n",
      "     weighted avg       0.80      0.79      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vectorized = Pipeline([\n",
    "    ('vec', CountVectorizer(ngram_range=(1, 2), max_features=20_000)),\n",
    "    ('scaler', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(solver='saga', tol=1e-3, max_iter=100, class_weight='balanced', random_state=42))\n",
    "])\n",
    "count_vectorized.fit(X_train, y_train)\n",
    "y_val_pred = count_vectorized.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе подобрал параметры так, чтобы получить базовое качество архитектуры\n",
    " - **CountVectorizer**\n",
    "   - `ngram_range=(1, 2)`: берем юниграммы и биграммы, дабы задействовать информацию о контексте слова\n",
    "   - `max_features=20_000`: описал выше  \n",
    " - **LogisticRegression**\n",
    "   - `solver='saga'`: имперически луше работает с разряженными данными и поддерживает мульти класс\n",
    "   - `tol=1e-3, max_iter=100`: параметры не, которые не влияют на ход обучения, только ставят ограничения на количество итераций, подобраны так, чтоб не было `ConvergenceWarning: The max_iter was reached which means the coef_ did not converge` Так как у нас данные разряженные, то это может влиять на сходимость задачи, по этому я явно ослабил требование \"хорошо обученной\" модели в контексте ошибки на обучающей выборке (параметром `tol`) и сохранил дефолтное значение `max_iter` (до максимума все равно не дошло обучение)  \n",
    "    - `class_weight='balanced'`: так как в данных разница в частотности классов доходит до порядков, то имеет смысл сбалансировать веса классов \n",
    "\n",
    "p. s. `MaxAbsScaler` не меняет форму распределения и сохраняет нули (и разрежённость соответственно), однако полезен для логистической регрессии, дабы улучшить численную сходимость задачи. Можно было использовать, например, l1/l2 нормализацию, однако такая нормализация меняет форму распределения, а мы в домашке хотим посмотреть на перфоманс чистых векторизаторов (как я понял)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            other       0.42      0.63      0.50       434\n",
      "      Бывший СССР       0.79      0.86      0.82      1420\n",
      "              Дом       0.75      0.88      0.81       578\n",
      "         Из жизни       0.53      0.75      0.62       743\n",
      "   Интернет и СМИ       0.73      0.73      0.73      1236\n",
      "         Культура       0.86      0.88      0.87      1468\n",
      "              Мир       0.82      0.78      0.80      3699\n",
      "  Наука и техника       0.82      0.83      0.82      1426\n",
      "           Россия       0.85      0.72      0.78      4374\n",
      "Силовые структуры       0.48      0.71      0.57       532\n",
      "            Спорт       0.95      0.96      0.96      1726\n",
      "         Ценности       0.97      0.56      0.71       216\n",
      "        Экономика       0.85      0.82      0.84      2148\n",
      "\n",
      "         accuracy                           0.79     20000\n",
      "        macro avg       0.76      0.78      0.76     20000\n",
      "     weighted avg       0.81      0.79      0.80     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorized = Pipeline([\n",
    "    ('vec', TfidfVectorizer(ngram_range=(1, 2), max_features=20_000)),\n",
    "    ('clf', LogisticRegression(solver='saga', tol=1e-3, max_iter=100, class_weight='balanced', random_state=42))\n",
    "])\n",
    "tfidf_vectorized.fit(X_train, y_train)\n",
    "y_val_pred = tfidf_vectorized.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результатам можно понять\n",
    "1. Метрики TfIdf не сиольно лучше, чем Count. Это может говорить о том, что idf балансировка не дает качества. Можно предположить, что данные и так достаточно сбалансированы.\n",
    "2. f1 на большистве классов высокое для обоих подходов, однако есть 3 класса с маленьким весом, на которы качество ниже. При подборе гипперпараметром можно попробовать class_weight='balanced' для одинакого веса каждого класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 8 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/education/dl_nlp/.venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 1, 'clf__class_weight': None, 'clf__penalty': 'l2'}\n",
      "Best CV score: 0.871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "\n",
    "params = {\n",
    "    'clf__C': [1e-1, 1], # шатаем l2 регуляризацию\n",
    "    'clf__penalty': ['l2', 'elasticnet'], # шатаем l2 регуляризацию\n",
    "    'clf__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "class CustomSplit(BaseCrossValidator):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def split(self, X=None, y=None, groups=None):\n",
    "        yield (np.arange(len(self.X_train)), np.arange(len(self.X_test)))\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return 1\n",
    "\n",
    "custom_split = CustomSplit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    tfidf_vectorized,\n",
    "    params,\n",
    "    cv=custom_split,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    grid.fit(pd.concat((X_train, X_test)), pd.concat((y_train, y_test)))\n",
    "\n",
    "print(f'Best params: {grid.best_params_}')\n",
    "print(f'Best CV score: {grid.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            other       0.74      0.35      0.48       434\n",
      "      Бывший СССР       0.82      0.81      0.82      1420\n",
      "              Дом       0.86      0.80      0.83       578\n",
      "         Из жизни       0.70      0.54      0.61       743\n",
      "   Интернет и СМИ       0.78      0.69      0.73      1236\n",
      "         Культура       0.87      0.89      0.88      1468\n",
      "              Мир       0.78      0.84      0.81      3699\n",
      "  Наука и техника       0.83      0.85      0.84      1426\n",
      "           Россия       0.78      0.84      0.81      4374\n",
      "Силовые структуры       0.72      0.40      0.52       532\n",
      "            Спорт       0.96      0.96      0.96      1726\n",
      "         Ценности       0.93      0.74      0.82       216\n",
      "        Экономика       0.82      0.86      0.84      2148\n",
      "\n",
      "         accuracy                           0.81     20000\n",
      "        macro avg       0.81      0.74      0.76     20000\n",
      "     weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = grid.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем не сильный прирост, что может быть связано с малым `max_iter` (но не хочеться ждать 1000 лет) + сложностями численной оптимизации sparsed векторов. Как дополнение, можно использовать l2 Normalizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
