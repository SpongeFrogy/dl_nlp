{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd98adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corus import load_ne5\n",
    "import os\n",
    "import zipfile\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report\n",
    "import evaluate\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer,  AutoModelForTokenClassification, AutoModelForMaskedLM, DataCollatorForLanguageModeling, DataCollatorForTokenClassification, pipeline\n",
    "import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b1ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'cointegrated/rubert-tiny2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c57e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802cec2",
   "metadata": {},
   "source": [
    "# 1. Train NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11d691",
   "metadata": {},
   "source": [
    "## 1.1 Load Collections5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478eb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already loaded.\n"
     ]
    }
   ],
   "source": [
    "path = 'collection5.zip'\n",
    "directory = 'Collection5'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    !wget http://www.labinform.ru/pub/named_entities/collection5.zip\n",
    "    with zipfile.ZipFile(path, 'r') as f:\n",
    "        f.extractall()\n",
    "        print(\"Unzipped.\")\n",
    "else:\n",
    "    print(\"Already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473e132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for recode in load_ne5(directory):\n",
    "    records.append(\n",
    "        {\n",
    "            'text': recode.text,\n",
    "            'id': recode.id,\n",
    "            'spans': recode.spans\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e579784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(text: str, spans: list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    labels = ['O'] * len(tokens)\n",
    "    for span in spans:\n",
    "        start, end, label = span.start, span.stop, span.type\n",
    "        span_tokens = tokenizer.tokenize(text[start:end])\n",
    "        for j in range(len(tokens)):\n",
    "            if tokens[j:j+len(span_tokens)] == span_tokens:\n",
    "                labels[j] = f'B-{label}'\n",
    "                labels[j+1:j+len(span_tokens)\n",
    "                       ] = [f'I-{label}'] * (len(span_tokens) - 1)\n",
    "    return {\"text\": text, \"tokens\": tokens, \"ner_tags\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c07187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2306 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = pd.DataFrame(list(map(lambda x: labeling(x['text'], x['spans']), records)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ed793a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В Санкт-Петербурге создают госкорпорацию промы...</td>\n",
       "      <td>[В, Санкт, -, Петербурге, создают, госкорпора,...</td>\n",
       "      <td>[O, B-LOC, I-LOC, I-LOC, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Генпрокуратура Египта распорядилась закрыть оп...</td>\n",
       "      <td>[Генпроку, ##ратура, Египта, распоряди, ##лась...</td>\n",
       "      <td>[B-ORG, I-ORG, B-GEOPOLIT, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Глава Скотланд-Ярда ушел в отставку из за скан...</td>\n",
       "      <td>[Глава, Ско, ##тла, ##нд, -, Яр, ##да, ушел, в...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Д.Медведев уволил главу Ярославской области\\r\\...</td>\n",
       "      <td>[Д, ., Медведев, уволил, главу, Ярославской, о...</td>\n",
       "      <td>[B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Президент Мексики предложил переименовать стра...</td>\n",
       "      <td>[Президент, Мексики, предложил, переименов, ##...</td>\n",
       "      <td>[O, B-GEOPOLIT, O, O, O, O, O, B-GEOPOLIT, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Новым главой МИД Египта стал Мухаммед Камель А...</td>\n",
       "      <td>[Новым, главой, МИД, Египта, стал, Мухаммед, К...</td>\n",
       "      <td>[O, O, B-ORG, B-GEOPOLIT, O, B-PER, I-PER, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Ювентус\" обыграл \"Милан\" в Кубке Италии\\r\\n\"Ю...</td>\n",
       "      <td>[\", Ювентус, \", обыграл, \", Милан, \", в, Кубке...</td>\n",
       "      <td>[O, B-ORG, O, O, O, B-ORG, O, O, O, B-GEOPOLIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>\\r\\nЛавров сохранил пост главы комиссии по вза...</td>\n",
       "      <td>[Лавров, сохранил, пост, главы, комиссии, по, ...</td>\n",
       "      <td>[B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Чавес назначил нового вице-президента\\r\\nПрези...</td>\n",
       "      <td>[Ч, ##аве, ##с, назначил, нового, вице, -, пре...</td>\n",
       "      <td>[B-PER, I-PER, I-PER, O, O, O, O, O, O, B-GEOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Над едиными учебниками нужно еще много работат...</td>\n",
       "      <td>[Над, единым, ##и, учебника, ##ми, нужно, еще,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, I-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    В Санкт-Петербурге создают госкорпорацию промы...   \n",
       "1    Генпрокуратура Египта распорядилась закрыть оп...   \n",
       "2    Глава Скотланд-Ярда ушел в отставку из за скан...   \n",
       "3    Д.Медведев уволил главу Ярославской области\\r\\...   \n",
       "4    Президент Мексики предложил переименовать стра...   \n",
       "..                                                 ...   \n",
       "995  Новым главой МИД Египта стал Мухаммед Камель А...   \n",
       "996  \"Ювентус\" обыграл \"Милан\" в Кубке Италии\\r\\n\"Ю...   \n",
       "997  \\r\\nЛавров сохранил пост главы комиссии по вза...   \n",
       "998  Чавес назначил нового вице-президента\\r\\nПрези...   \n",
       "999  Над едиными учебниками нужно еще много работат...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [В, Санкт, -, Петербурге, создают, госкорпора,...   \n",
       "1    [Генпроку, ##ратура, Египта, распоряди, ##лась...   \n",
       "2    [Глава, Ско, ##тла, ##нд, -, Яр, ##да, ушел, в...   \n",
       "3    [Д, ., Медведев, уволил, главу, Ярославской, о...   \n",
       "4    [Президент, Мексики, предложил, переименов, ##...   \n",
       "..                                                 ...   \n",
       "995  [Новым, главой, МИД, Египта, стал, Мухаммед, К...   \n",
       "996  [\", Ювентус, \", обыграл, \", Милан, \", в, Кубке...   \n",
       "997  [Лавров, сохранил, пост, главы, комиссии, по, ...   \n",
       "998  [Ч, ##аве, ##с, назначил, нового, вице, -, пре...   \n",
       "999  [Над, единым, ##и, учебника, ##ми, нужно, еще,...   \n",
       "\n",
       "                                              ner_tags  \n",
       "0    [O, B-LOC, I-LOC, I-LOC, O, O, O, O, O, O, O, ...  \n",
       "1    [B-ORG, I-ORG, B-GEOPOLIT, O, O, O, O, O, O, O...  \n",
       "2    [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...  \n",
       "3    [B-PER, I-PER, I-PER, O, O, B-LOC, I-LOC, O, B...  \n",
       "4    [O, B-GEOPOLIT, O, O, O, O, O, B-GEOPOLIT, O, ...  \n",
       "..                                                 ...  \n",
       "995  [O, O, B-ORG, B-GEOPOLIT, O, B-PER, I-PER, I-P...  \n",
       "996  [O, B-ORG, O, O, O, B-ORG, O, O, O, B-GEOPOLIT...  \n",
       "997  [B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O, B...  \n",
       "998  [B-PER, I-PER, I-PER, O, O, O, O, O, O, B-GEOP...  \n",
       "999  [O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, I-...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ec872",
   "metadata": {},
   "source": [
    "Загрузили датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cacfe",
   "metadata": {},
   "source": [
    "## 1.2 Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510d767",
   "metadata": {},
   "source": [
    "Разделяем данные на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c01d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_SAMPLE_SIZE = 0.2\n",
    "\n",
    "raw_train, raw_test = train_test_split(raw_dataset, test_size=TEST_SAMPLE_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd51818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq: 0.7777, Test freq: 0.7774 Tag: O: \n",
      "Train freq: 0.0049, Test freq: 0.0049 Tag: B-MEDIA: \n",
      "Train freq: 0.0055, Test freq: 0.0054 Tag: I-MEDIA: \n",
      "Train freq: 0.0107, Test freq: 0.0103 Tag: B-LOC: \n",
      "Train freq: 0.0376, Test freq: 0.0378 Tag: B-PER: \n",
      "Train freq: 0.0758, Test freq: 0.0767 Tag: I-PER: \n",
      "Train freq: 0.0133, Test freq: 0.0134 Tag: B-GEOPOLIT: \n",
      "Train freq: 0.0024, Test freq: 0.0024 Tag: I-GEOPOLIT: \n",
      "Train freq: 0.0231, Test freq: 0.0232 Tag: B-ORG: \n",
      "Train freq: 0.0368, Test freq: 0.0370 Tag: I-ORG: \n",
      "Train freq: 0.0122, Test freq: 0.0116 Tag: I-LOC: \n"
     ]
    }
   ],
   "source": [
    "assert set(raw_train['ner_tags'].sum()) == set(raw_test['ner_tags'].sum()), 'Some of tags not in both train/test'\n",
    "\n",
    "train_tags_cnt = Counter(raw_train['ner_tags'].sum())\n",
    "test_tags_cnt = Counter(raw_dataset['ner_tags'].sum())\n",
    "\n",
    "for tag in train_tags_cnt.keys():\n",
    "    print(f'Train freq: {train_tags_cnt[tag] / sum(train_tags_cnt.values()):.4f}, Test freq: {test_tags_cnt[tag] / sum(test_tags_cnt.values()):.4f} Tag: {tag}: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b0bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = sorted(train_tags_cnt.keys())\n",
    "id2tag = {i: tag for i, tag in enumerate(tags)}\n",
    "tag2id = {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282300f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = datasets.Features({\n",
    "    \"tokens\": datasets.Sequence(feature=datasets.Value(\"string\")),\n",
    "    \"ner_tags\": datasets.Sequence(feature=datasets.ClassLabel(names=tags))\n",
    "})\n",
    "\n",
    "train = datasets.Dataset.from_dict({'tokens': raw_train['tokens'], 'ner_tags': raw_train['ner_tags']}, features=features)\n",
    "test = datasets.Dataset.from_dict({'tokens': raw_test['tokens'], 'ner_tags': raw_test['ner_tags']}, features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a813e",
   "metadata": {},
   "source": [
    "Разбили данные на train/test \n",
    "\n",
    "Удастоверились, что частотность меток в train/test примерно одинаковое  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e4c31",
   "metadata": {},
   "source": [
    "## 1.3 Train model for NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9423ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd0bf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837cc0080d1e43d894fe6c7145e6fc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983028e4a441410f8086c0bf9329a97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "simple_trainer = Trainer(\n",
    "    model=simple_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6338ea22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.03      0.45      0.06       388\n",
      "         LOC       0.01      0.10      0.01       235\n",
      "       MEDIA       0.01      0.06      0.02       153\n",
      "         ORG       0.00      0.01      0.00       540\n",
      "         PER       0.01      0.02      0.01       934\n",
      "\n",
      "   micro avg       0.02      0.10      0.03      2250\n",
      "   macro avg       0.01      0.13      0.02      2250\n",
      "weighted avg       0.01      0.10      0.02      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#init metrics\n",
    "def report(trainer: Trainer):\n",
    "    predictions, labels, loss = trainer.predict(tokenized_test)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[id2tag[p] for p, l in zip(pred, label) if l != -100] \n",
    "                for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    return classification_report(true_labels, pred_labels)\n",
    "\n",
    "rprt = report(simple_trainer)\n",
    "reports['simple_model_before_train'] = rprt\n",
    "\n",
    "print(rprt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3d56c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.016602</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694065</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.548105</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471383</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423856</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.385486</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362924</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.348801</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.340781</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.337425</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = simple_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6461b2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.80      0.84      0.82       388\n",
      "         LOC       0.53      0.50      0.52       235\n",
      "       MEDIA       0.48      0.27      0.34       153\n",
      "         ORG       0.39      0.48      0.43       540\n",
      "         PER       0.51      0.65      0.57       934\n",
      "\n",
      "   micro avg       0.52      0.60      0.56      2250\n",
      "   macro avg       0.54      0.55      0.54      2250\n",
      "weighted avg       0.53      0.60      0.56      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rprt = report(simple_trainer)\n",
    "reports['simple_model_after_train'] = rprt\n",
    "\n",
    "print(rprt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bed3ad",
   "metadata": {},
   "source": [
    "Сейчас и в дальнейшем смотрим на ___F1 weighted___ \n",
    "\n",
    "Вимим, что модель обучилась, причем относительно нормально \n",
    "\n",
    "Далее будем пытаться улучшить результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc83e5",
   "metadata": {},
   "source": [
    "# 2. Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa30de",
   "metadata": {},
   "source": [
    "## 2.1 MLM->NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2233479",
   "metadata": {},
   "source": [
    "Загружаем модель в режиме MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82a96a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85dc88a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1e5168e564159b250d79c38418f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlm_train_text = datasets.Dataset.from_dict({\"text\": raw_train['text'].tolist()})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "mlm_train = mlm_train_text.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd223dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLM_PROBABILITY = 0.15\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=MLM_PROBABILITY,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "662f2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask(text: str) -> tuple[str, str]:\n",
    "    words = text.split()\n",
    "    masked_id = np.random.randint(0, len(words) - 1)\n",
    "    masked_word = words[masked_id]\n",
    "    words[masked_id] = \"[MASK]\"\n",
    "    return \" \".join(words), masked_word\n",
    "\n",
    "mlm_test = [add_mask(text) for text in raw_test['text'].tolist()]\n",
    "\n",
    "def calculate_top_recalls(mlm_model, ns=[1, 5]):\n",
    "    correct = [0]*len(ns)\n",
    "    total = 0\n",
    "    mask_filler = pipeline(\"fill-mask\", model=mlm_model, tokenizer=tokenizer)\n",
    "    for text, masked_word in mlm_test:\n",
    "        predictions = mask_filler(text, top_k=max(ns))\n",
    "        predicted_words = [predictions[i]['token_str'] for i in range(max(ns))]\n",
    "        for i, n in enumerate(ns):\n",
    "            if masked_word in predicted_words[:n]:\n",
    "                correct[i] += 1 \n",
    "        total += 1\n",
    "    return [c / total for c in correct]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dd89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial MLM accuracy: 0.285, Recall@5: 0.435\n"
     ]
    }
   ],
   "source": [
    "acc, rec = calculate_top_recalls(mlm_model, ns=[1, 5])\n",
    "\n",
    "print(f'Initial MLM accuracy: {acc}, Recall@5: {rec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73982069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 08:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlm_training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "mlm_trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=mlm_training_args,\n",
    "    train_dataset=mlm_train,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "_ = mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9d3d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model.save_pretrained(\"./mlm-rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2e7c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained MLM accuracy: 0.315, Recall@5: 0.47\n"
     ]
    }
   ],
   "source": [
    "acc, rec = calculate_top_recalls(mlm_model, ns=[1, 5])\n",
    "\n",
    "print(f'Trained MLM accuracy: {acc}, Recall@5: {rec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ba032",
   "metadata": {},
   "source": [
    "Видим, что уже изночально модель не ужасно решает MLM задачу\n",
    "\n",
    "\n",
    "Также по разнице в метриках до\\после обучения можно понять, что модель немного обучилась\n",
    "\n",
    "Дальшейшее решение задачи mlm затруднительно, так как количество данных низкое для данной задачи "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "form_mlm_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    './mlm-rubert-tiny2',\n",
    "    num_labels=len(tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1230bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_ner_res\",\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "from_mlm_trainer = Trainer(\n",
    "    model=form_mlm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "723789ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.03      0.36      0.05       388\n",
      "         LOC       0.01      0.08      0.02       235\n",
      "       MEDIA       0.00      0.01      0.00       153\n",
      "         ORG       0.00      0.00      0.00       540\n",
      "         PER       0.01      0.02      0.01       934\n",
      "\n",
      "   micro avg       0.01      0.08      0.02      2250\n",
      "   macro avg       0.01      0.10      0.02      2250\n",
      "weighted avg       0.01      0.08      0.01      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rprt = report(from_mlm_trainer)\n",
    "reports['form_mlm_before_train'] = rprt\n",
    "\n",
    "print(rprt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c59df8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976450</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645057</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516093</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.443613</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.397509</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363229</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343079</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.330035</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.320627</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = from_mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19db1ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.86      0.82      0.84       388\n",
      "         LOC       0.59      0.47      0.53       235\n",
      "       MEDIA       0.69      0.42      0.52       153\n",
      "         ORG       0.40      0.54      0.46       540\n",
      "         PER       0.51      0.66      0.57       934\n",
      "\n",
      "   micro avg       0.54      0.62      0.58      2250\n",
      "   macro avg       0.61      0.58      0.58      2250\n",
      "weighted avg       0.56      0.62      0.58      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rprt = report(from_mlm_trainer)\n",
    "reports['form_mlm_after_train'] = rprt\n",
    "\n",
    "print(rprt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43154a7b",
   "metadata": {},
   "source": [
    "Видно, что дообучение в mlm дало прирост в метриках, но незначительный (так как метрики MLM задачи также не очень большие)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ec912",
   "metadata": {},
   "source": [
    "## 2.2 Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd deeppavlov && bash run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ec9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_tokens, ag_tags = pickle.load(open('deeppavlov/ner.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f505ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Названы регионы России с \\xa0 самой высокой смертностью от \\xa0 рака',\n",
       " 'O O S-LOC O O O O O O O O')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(ag_tokens[0]), \" \".join(ag_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c9385e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8840 :O\n",
      "0.0449 :S-LOC\n",
      "0.0179 :S-ORG\n",
      "0.0273 :S-PER\n",
      "0.0027 :B-LOC\n",
      "0.0028 :E-LOC\n",
      "0.0033 :B-ORG\n",
      "0.0005 :I-ORG\n",
      "0.0033 :E-ORG\n",
      "0.0064 :B-PER\n",
      "0.0066 :E-PER\n",
      "0.0001 :I-LOC\n",
      "0.0002 :I-PER\n"
     ]
    }
   ],
   "source": [
    "ag_tags_cnt = Counter()\n",
    "for t in ag_tags:\n",
    "    ag_tags_cnt.update(t)\n",
    "_total = sum(ag_tags_cnt.values())\n",
    "for tag, count in ag_tags_cnt.items():\n",
    "    print(f'{count / _total:.4f} :{tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091b84f",
   "metadata": {},
   "source": [
    "Видим, что больше всего бы дадим данныз для локаций (`LOC`) и организаций (`ORG`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16dea4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeppavlov2collections_ner_tags = {\n",
    "    'O': 'O',\n",
    "    'S-LOC': 'LOC',\n",
    "    'S-PER': 'PER',\n",
    "    'S-ORG': 'ORG',\n",
    "    'E-PER': 'PER',\n",
    "    'B-PER': 'PER',\n",
    "    'B-ORG': 'ORG',\n",
    "    'E-ORG': 'ORG',\n",
    "    'E-LOC': 'LOC',\n",
    "    'B-LOC': 'LOC',\n",
    "    'I-ORG': 'ORG',\n",
    "    'I-PER': 'PER',\n",
    "    'I-LOC': 'LOC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e7d5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(slots=True)\n",
    "class Span:\n",
    "    start: int\n",
    "    stop: int\n",
    "    type: str\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class Row:\n",
    "    text: str\n",
    "    spans: list[Span]\n",
    "\n",
    "def _resolve_spans(spans: list[Span]) -> list[Span]:\n",
    "    i = 0\n",
    "\n",
    "    while i < len(spans) - 1:\n",
    "        if spans[i].stop + 1 == spans[i+1].start and spans[i].type == spans[i+1].type:\n",
    "            spans[i].stop = spans[i+1].stop\n",
    "            spans.pop(i+1)\n",
    "            _resolve_spans(spans)\n",
    "        i += 1\n",
    "    return spans\n",
    "    \n",
    "\n",
    "def make_dataset_row(tokens: list[str], tags: list[str]) -> Optional[Row]:\n",
    "    spans = []\n",
    "    curr_len = 0\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag == 'O':\n",
    "            curr_len += len(token) + 1\n",
    "        else:\n",
    "            spans.append(Span(start=curr_len, stop=curr_len + len(token), type=deeppavlov2collections_ner_tags[tag]))\n",
    "            curr_len += len(token) + 1\n",
    "    if len(spans) == 0:\n",
    "        return None\n",
    "    return Row(text=\" \".join(tokens), spans=_resolve_spans(spans))\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "678e2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = [\n",
    "    Span(start=0, stop=1, type='PER'),\n",
    "    Span(start=2, stop=3, type='PER'),\n",
    "\n",
    "    Span(start=24, stop=25, type='LOC'),\n",
    "\n",
    "    Span(start=26, stop=27, type='PER'),\n",
    " \n",
    "    Span(start=32, stop=33, type='PER'),\n",
    "    Span(start=34, stop=35, type='PER'),\n",
    "]\n",
    "assert _resolve_spans(spans) == [Span(start=0, stop=3, type='PER'), Span(start=24, stop=25, type='LOC'), Span(start=26, stop=27, type='PER'), Span(start=32, stop=35, type='PER')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ddf1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentation = []\n",
    "\n",
    "for tkn, tg in zip(ag_tokens, ag_tags):\n",
    "    row = make_dataset_row(tkn, tg)\n",
    "    if row is not None:\n",
    "        argumentation.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce95ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentation_raw_dataset = pd.DataFrame(list(map(lambda x: labeling(x.text, x.spans), argumentation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6090b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Названы регионы России с   самой высокой смерт...</td>\n",
       "      <td>[Названы, регионы, России, с, самой, высокой, ...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Австрия не   представила доказательств вины ро...</td>\n",
       "      <td>[Австрия, не, представила, доказательств, вины...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В США раскрыли сумму расходов на   расследован...</td>\n",
       "      <td>[В, США, раскрыли, сумму, расходов, на, рассле...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Хакеры рассказали о   планах Великобритании за...</td>\n",
       "      <td>[Хак, ##еры, рассказали, о, планах, Великобрит...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, O, O, B-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Архиепископ канонической УПЦ отказался прийти ...</td>\n",
       "      <td>[Архие, ##пископ, канони, ##ческой, УПЦ, отказ...</td>\n",
       "      <td>[O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>МИД Сирии определило предназначение С - 300</td>\n",
       "      <td>[МИД, Сирии, определило, предназначение, С, -,...</td>\n",
       "      <td>[B-ORG, B-LOC, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6201</th>\n",
       "      <td>Памятный знак белому генералу на   родине Лени...</td>\n",
       "      <td>[Памят, ##ный, знак, белом, ##у, генералу, на,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-PER, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6202</th>\n",
       "      <td>В Польше заявили о   конфликте идентичностей с...</td>\n",
       "      <td>[В, Польше, заявили, о, конфликте, иденти, ##ч...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, B-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6203</th>\n",
       "      <td>Возможность морской блокады России назвали фан...</td>\n",
       "      <td>[Возможность, морской, блокады, России, назвал...</td>\n",
       "      <td>[O, O, O, B-LOC, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>Майк Тайсон приехал в   Индию и   восхвалил ни...</td>\n",
       "      <td>[Майк, Тайсон, приехал, в, Индию, и, восхвал, ...</td>\n",
       "      <td>[B-PER, I-PER, O, O, B-LOC, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6205 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Названы регионы России с   самой высокой смерт...   \n",
       "1     Австрия не   представила доказательств вины ро...   \n",
       "2     В США раскрыли сумму расходов на   расследован...   \n",
       "3     Хакеры рассказали о   планах Великобритании за...   \n",
       "4     Архиепископ канонической УПЦ отказался прийти ...   \n",
       "...                                                 ...   \n",
       "6200        МИД Сирии определило предназначение С - 300   \n",
       "6201  Памятный знак белому генералу на   родине Лени...   \n",
       "6202  В Польше заявили о   конфликте идентичностей с...   \n",
       "6203  Возможность морской блокады России назвали фан...   \n",
       "6204  Майк Тайсон приехал в   Индию и   восхвалил ни...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [Названы, регионы, России, с, самой, высокой, ...   \n",
       "1     [Австрия, не, представила, доказательств, вины...   \n",
       "2     [В, США, раскрыли, сумму, расходов, на, рассле...   \n",
       "3     [Хак, ##еры, рассказали, о, планах, Великобрит...   \n",
       "4     [Архие, ##пископ, канони, ##ческой, УПЦ, отказ...   \n",
       "...                                                 ...   \n",
       "6200  [МИД, Сирии, определило, предназначение, С, -,...   \n",
       "6201  [Памят, ##ный, знак, белом, ##у, генералу, на,...   \n",
       "6202  [В, Польше, заявили, о, конфликте, иденти, ##ч...   \n",
       "6203  [Возможность, морской, блокады, России, назвал...   \n",
       "6204  [Майк, Тайсон, приехал, в, Индию, и, восхвал, ...   \n",
       "\n",
       "                                               ner_tags  \n",
       "0                    [O, O, B-LOC, O, O, O, O, O, O, O]  \n",
       "1                          [B-LOC, O, O, O, O, O, O, O]  \n",
       "2                 [O, B-LOC, O, O, O, O, O, O, O, O, O]  \n",
       "3                   [O, O, O, O, O, B-LOC, O, O, B-LOC]  \n",
       "4     [O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O,...  \n",
       "...                                                 ...  \n",
       "6200                      [B-ORG, B-LOC, O, O, O, O, O]  \n",
       "6201           [O, O, O, O, O, O, O, O, B-PER, O, O, O]  \n",
       "6202             [O, B-LOC, O, O, O, O, O, O, O, B-LOC]  \n",
       "6203                          [O, O, O, B-LOC, O, O, O]  \n",
       "6204         [B-PER, I-PER, O, O, B-LOC, O, O, O, O, O]  \n",
       "\n",
       "[6205 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argumentation_raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a2488be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Названы регионы России с   самой высокой смерт...</td>\n",
       "      <td>[Названы, регионы, России, с, самой, высокой, ...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Австрия не   представила доказательств вины ро...</td>\n",
       "      <td>[Австрия, не, представила, доказательств, вины...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В США раскрыли сумму расходов на   расследован...</td>\n",
       "      <td>[В, США, раскрыли, сумму, расходов, на, рассле...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Хакеры рассказали о   планах Великобритании за...</td>\n",
       "      <td>[Хак, ##еры, рассказали, о, планах, Великобрит...</td>\n",
       "      <td>[O, O, O, O, O, B-LOC, O, O, B-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Архиепископ канонической УПЦ отказался прийти ...</td>\n",
       "      <td>[Архие, ##пископ, канони, ##ческой, УПЦ, отказ...</td>\n",
       "      <td>[O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Экс-глава московского ОМОНа возглавил столичны...</td>\n",
       "      <td>[Экс, -, глава, московского, ОМОН, ##а, возгла...</td>\n",
       "      <td>[O, O, O, O, B-ORG, I-ORG, O, O, B-ORG, I-ORG,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Обама назначил нового главу своей администраци...</td>\n",
       "      <td>[Обама, назначил, нового, главу, своей, админи...</td>\n",
       "      <td>[B-PER, O, O, O, O, O, O, B-GEOPOLIT, B-PER, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Центр ядерной медицины и мордовский могильник:...</td>\n",
       "      <td>[Центр, ядерной, медицины, и, морд, ##овский, ...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, O, O, B-MEDIA, B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Д.Медведев 18-19 января посетит Иорданское Хаш...</td>\n",
       "      <td>[Д, ., Медведев, 18, -, 19, января, посетит, И...</td>\n",
       "      <td>[B-PER, I-PER, I-PER, O, O, O, O, O, B-GEOPOLI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Директор Центрального разведывательного управл...</td>\n",
       "      <td>[Директор, Центрального, разведыват, ##ельного...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Названы регионы России с   самой высокой смерт...   \n",
       "1    Австрия не   представила доказательств вины ро...   \n",
       "2    В США раскрыли сумму расходов на   расследован...   \n",
       "3    Хакеры рассказали о   планах Великобритании за...   \n",
       "4    Архиепископ канонической УПЦ отказался прийти ...   \n",
       "..                                                 ...   \n",
       "106  Экс-глава московского ОМОНа возглавил столичны...   \n",
       "270  Обама назначил нового главу своей администраци...   \n",
       "860  Центр ядерной медицины и мордовский могильник:...   \n",
       "435  Д.Медведев 18-19 января посетит Иорданское Хаш...   \n",
       "102  Директор Центрального разведывательного управл...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [Названы, регионы, России, с, самой, высокой, ...   \n",
       "1    [Австрия, не, представила, доказательств, вины...   \n",
       "2    [В, США, раскрыли, сумму, расходов, на, рассле...   \n",
       "3    [Хак, ##еры, рассказали, о, планах, Великобрит...   \n",
       "4    [Архие, ##пископ, канони, ##ческой, УПЦ, отказ...   \n",
       "..                                                 ...   \n",
       "106  [Экс, -, глава, московского, ОМОН, ##а, возгла...   \n",
       "270  [Обама, назначил, нового, главу, своей, админи...   \n",
       "860  [Центр, ядерной, медицины, и, морд, ##овский, ...   \n",
       "435  [Д, ., Медведев, 18, -, 19, января, посетит, И...   \n",
       "102  [Директор, Центрального, разведыват, ##ельного...   \n",
       "\n",
       "                                              ner_tags  \n",
       "0                   [O, O, B-LOC, O, O, O, O, O, O, O]  \n",
       "1                         [B-LOC, O, O, O, O, O, O, O]  \n",
       "2                [O, B-LOC, O, O, O, O, O, O, O, O, O]  \n",
       "3                  [O, O, O, O, O, B-LOC, O, O, B-LOC]  \n",
       "4    [O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O,...  \n",
       "..                                                 ...  \n",
       "106  [O, O, O, O, B-ORG, I-ORG, O, O, B-ORG, I-ORG,...  \n",
       "270  [B-PER, O, O, O, O, O, O, B-GEOPOLIT, B-PER, I...  \n",
       "860  [B-LOC, O, O, O, O, O, O, O, O, O, B-MEDIA, B-...  \n",
       "435  [B-PER, I-PER, I-PER, O, O, O, O, O, B-GEOPOLI...  \n",
       "102  [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...  \n",
       "\n",
       "[7005 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_raw_train = pd.concat([argumentation_raw_dataset, raw_train], axis=0)\n",
    "augmented_raw_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "777b3d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c38708aeeb14eb1b6007b8f7dc39084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = datasets.Features({\n",
    "    \"tokens\": datasets.Sequence(feature=datasets.Value(\"string\")),\n",
    "    \"ner_tags\": datasets.Sequence(feature=datasets.ClassLabel(names=tags))\n",
    "})\n",
    "\n",
    "augmented_train = datasets.Dataset.from_dict({'tokens': augmented_raw_train['tokens'], 'ner_tags': augmented_raw_train['ner_tags']}, features=features)\n",
    "\n",
    "tokenized_augmented_train = augmented_train.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=len(tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d2b79a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4380' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4380/4380 01:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.291468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.237805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.214048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.196375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.188109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.184297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.180256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.178005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.177598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "augmented_trainer = Trainer(\n",
    "    model=augmented_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_augmented_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "_ = augmented_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed3c1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.89      0.87      0.88       388\n",
      "         LOC       0.66      0.78      0.72       235\n",
      "       MEDIA       0.79      0.79      0.79       153\n",
      "         ORG       0.57      0.71      0.63       540\n",
      "         PER       0.66      0.69      0.67       934\n",
      "\n",
      "   micro avg       0.68      0.74      0.71      2250\n",
      "   macro avg       0.71      0.77      0.74      2250\n",
      "weighted avg       0.69      0.74      0.71      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rprt = report(augmented_trainer)\n",
    "reports['augmented_after_train'] = rprt\n",
    "\n",
    "print(rprt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dcc91",
   "metadata": {},
   "source": [
    "Получаем сильный прирост в метриках. Основной гипотезой считаю, что мы дали значительно больше данных и по этому общее распределение в датасете о языке (гениральной совокупности в нашем случае) стало качественее "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8307c",
   "metadata": {},
   "source": [
    "# 3. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8df33f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________Simple Model Before Train______________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.03      0.45      0.06       388\n",
      "         LOC       0.01      0.10      0.01       235\n",
      "       MEDIA       0.01      0.06      0.02       153\n",
      "         ORG       0.00      0.01      0.00       540\n",
      "         PER       0.01      0.02      0.01       934\n",
      "\n",
      "   micro avg       0.02      0.10      0.03      2250\n",
      "   macro avg       0.01      0.13      0.02      2250\n",
      "weighted avg       0.01      0.10      0.02      2250\n",
      "\n",
      "______________Simple Model After Train______________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.80      0.84      0.82       388\n",
      "         LOC       0.53      0.50      0.52       235\n",
      "       MEDIA       0.48      0.27      0.34       153\n",
      "         ORG       0.39      0.48      0.43       540\n",
      "         PER       0.51      0.65      0.57       934\n",
      "\n",
      "   micro avg       0.52      0.60      0.56      2250\n",
      "   macro avg       0.54      0.55      0.54      2250\n",
      "weighted avg       0.53      0.60      0.56      2250\n",
      "\n",
      "______________Form Mlm Before Train______________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.03      0.36      0.05       388\n",
      "         LOC       0.01      0.08      0.02       235\n",
      "       MEDIA       0.00      0.01      0.00       153\n",
      "         ORG       0.00      0.00      0.00       540\n",
      "         PER       0.01      0.02      0.01       934\n",
      "\n",
      "   micro avg       0.01      0.08      0.02      2250\n",
      "   macro avg       0.01      0.10      0.02      2250\n",
      "weighted avg       0.01      0.08      0.01      2250\n",
      "\n",
      "______________Form Mlm After Train______________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.86      0.82      0.84       388\n",
      "         LOC       0.59      0.47      0.53       235\n",
      "       MEDIA       0.69      0.42      0.52       153\n",
      "         ORG       0.40      0.54      0.46       540\n",
      "         PER       0.51      0.66      0.57       934\n",
      "\n",
      "   micro avg       0.54      0.62      0.58      2250\n",
      "   macro avg       0.61      0.58      0.58      2250\n",
      "weighted avg       0.56      0.62      0.58      2250\n",
      "\n",
      "______________Augmented After Train______________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.89      0.87      0.88       388\n",
      "         LOC       0.66      0.78      0.72       235\n",
      "       MEDIA       0.79      0.79      0.79       153\n",
      "         ORG       0.57      0.71      0.63       540\n",
      "         PER       0.66      0.69      0.67       934\n",
      "\n",
      "   micro avg       0.68      0.74      0.71      2250\n",
      "   macro avg       0.71      0.77      0.74      2250\n",
      "weighted avg       0.69      0.74      0.71      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, rprt in reports.items():\n",
    "    print(f\"______________{name.replace('_', ' ').title()}______________\", )\n",
    "    print(rprt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433e8a0",
   "metadata": {},
   "source": [
    "По итогу имеем\n",
    "1. В ner задаче без всего имеем ___(baseline)___:\n",
    "    - _F1 weighted_=0.56  \n",
    "    - лучше все предсказания на классе __GEOPOLIT__ (_f1-score_ = 0.82)\n",
    "    - на остальных картина не очень\n",
    "2. в ner задаче с моделью, предобученной в mlm имеем ___(from mlm)___\n",
    "    - незначительный прирост _F1 weighted_=0.58 ($\\Delta_{baseline} = 2\\% $)\n",
    "    - локально на классе __MEDIA__ с худшими метриками в ___baseline___ стали сильно лучше метрики \n",
    "    - слабый прирост объясняется сложностями в MLM задаче на малых данных\n",
    "3. в ner задаче с аугментацией имеем ___(augmented)___\n",
    "    - нучшие метрики _F1 weighted_=0.71 ($\\Delta_{baseline} = 15\\% $)\n",
    "    - значительно лучше предсказываем на всех классах\n",
    "    - сильный прирост объясняется хначительным увеличением тренировочных данных (лучшее представление о языке) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9eba1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
